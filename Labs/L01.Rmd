---
title: "STAT301-3 Data Science III Overview"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

# Question 1
\textbf{Provide a brief outline/overview of the steps involved in a supervised machine learning process.}

- Obtain data. Do quality check(s), etc.

- Split data. This is usually into two - a training set and a test set. Sometimes a third split is created, either for validation set approach or to use for exploratory data analysis (EDA). Stratify (usually by the outcome variable).

- Feature engineering. Create a recipe. Do any scaling, transformation, etc. that are necessary to get the data ready for model fitting. This include principle component analysis (PCA), missing data imputation, anything that needs to be done prior to fitting the models. 

- Set up cross-validation. V-fold cross validation with at least 2 repeats is usually optimal. Select a number of folds such that there are sufficient observations in each fold. Stratify (usually by the outcome variable).

- Set up models. Create tuning grids for each model's tuning parameters. 

- Tune all models across resamples.

- Collect the results of tuning. Determine which class of model performed best on your selected metric. Within that, determine the optimal tuning parameter values. 

- Fit your chosen (tuned) model to the entire training set. 

- Using the results from the training set, fit your chosen model to the entire testing set. 

- Check out how well your model did. 


# Question 2

\textbf{Explain the difference between supervised and unsupervised learning.}

The fundamental difference between the two is that, during supervised learning, the "machine" (or model) is \textit{supervised}. Like a teacher in a classroom supervises students' learning, letting them know when their answer is correct or incorrect, in supervised learning the user knows the truth for each observation and can determine whether or not the model is correct. The model parameters are updated based on its performance. 

By contrast, unsupervised learning is \textit{unsupervised}. Cluster analysis is a good example of this. In cluster analysis, models usually are not judged based on accuracy - the models are given the data and set loose to find patterns, or clusters. There is no right or wrong cluster solution. In general, any time models are learning from data and there is no "answer key", it's likely to be unsupervised learning. 

# Question 3

\textbf{In general, we can classify a model by its purpose into 1 of the 3 categories below. Provide a breif description of the goals of these models classes.}

1. \textit{Descriptive Models}: Models intended to describe trends in data - like drawing a line on a scatterplot to emphasize a pattern or detecting patterns in medical images. 

2. \textit{Inferential Models}: Models intended to test hypotheses about causal relationships and/or experiments - like whether administering a treatment causes a reduction in a symptoms. 

3. \textit{Predictive Models}: Models intended to predict an outcome variable as accurately as possible. 


# Question 4

\textbf{We can describe predictive models by how they were developed as being either mechanistic or empirically driven.}

- \textbf{What does it mean to be a mechanistic model?}

Mechanistic models are heavily based on assumptions made about the parameters and relationships between the parameters. Linear regression is an example of a mechanistic model. 

- \textbf{What does it mean to be an empirically driven model?}

Empirically driven models are generated from data and make very few, if any, assumptions about the parameters, etc. They are defined by the structure of their prediction(s). Random forest(s) are an example of empirically driven model.

- \textbf{How does the mechanistic and empirically driven model terminology relate to the parametric and nonparametric model terminology?}

Mechanistic models tend to be parametric; that is, they assume that the complexity of the data is bounded y a finite set of specific parameters. 

Empirically driven models tend to be nonparametric; which primarily means that they do not make such an assumption and do not have a fixed/finited set of parameters. 

- \textbf{In general, is a mechanistic or empirically driven model eaiser to understand? Explain.}

Mechanistic models are usually easier to understand. A linear regression, for example, is easier to understand and explain than a random forest. This is because the assumtpions and parameters are fixed and known, etc. 

- \textbf{How does the mechanistic and empirically driven model terminology related to the idea of model flexibility? That is, which would be more or less flexible than the other?}

Empirically driven models are generally much more flexible than mechanistic models (for reasons described above).

- \textbf{Describe the bias-variance trade-off when considering the use of a mechanistic or empirically driven model.}

Using a mechanistic model is like taking a fixed frame and trying to fit different data shapes into said frame. It results in lower variance - because the frame itself, or the model mechanisms, do not change very much - but bigger bias, because the data likely do not exactly fit the mechanisms of the model. 

On the other hand, using an empirically-drive model is like modling a frame to a data shape. It results in higher variance - because the mold differs depending on the data it was fit to - but lower bias, because the data themselves fit the mold very closely. 


# Question 5

\textbf{Explain the difference between a regression and  classification machine learning (ML) problems.}

The type of outcome variable being predicted determines whether an ML problem is a regression or classification problem. If the outcome variable is continuous, the ML problem is regression; if it is categorical, it is classification. 



# Question 6

\textbf{When splitting the data, why is it useful to stratify by the outcome/target variable?}

It is almost always useful to stratify by the outcome; this makes sure that all splits of the data will have the same (or very similar) outcome distribution. 

For example, if you do not stratify and the outcome is normally distributed, a split (random sample) is more likely to contain outcome values in the middle of the distribution, and may not be a representative sample. 

Stratifying is especially important with categorical outcomes, even more so if the outcome is imbalanced (one group is more common than another). 


# Question 7 

\textbf{Briefly describe how v-fold cross validation with repeats is used to estimate test RMSE. Also provide an explanation of why we use it.}

When we do V-fold cross-validation, we divide the training set into V folds. We then hold out one fold each time as a mini test set, or assessment set, and use the remaining folds as a mini tranining set. By fitting models to the mini training sets and testing them on the mini testing sets, we can get an idea of how the models will perform on ew data and (usually) get a good estimate of the model's RMSE on the test set.



# Question 8

\textbf{Briefly describe the model tuning and why we use it.}

Certain parameters - known as tuning parameters, or hyperparameters - are user specified; their values can not be optimized directly from the data. The results of the model(s) change depending on the values of their hyperparameters. We can improve (or worsen) the performance of the model(s) by changing hyperparameter values. We tune by estimating the model(s) multiple times with different values and comparing performance.


# Question 9

\textbf{What are two common performance metrics when dealing with a regression ML problem?}

Root mean squared error (RMSE) and $R^2$ are common performance metrics for regression. 

\textbf{What are two common performance metrics when dealing with a classification ML problem?}

The area under the receiver operating curve (AUC ROC) and accuracy are common performance metrics for a classification ML problem.


# Question 10

\textbf{A political candidate's campaign has detailed voter history data for their constituents. The campaign is interested in two questions:}

\textbf{1. Given a voter's profile/data how likely is it that they will vote in favor of the candidate?}

\textbf{2. How would a voter's likelihood of support for the candidate change if they had personal contact ith the candidate?}

\textbf{Classify each question/problem as either predictiv or inferential. Explain your reasoning for each.}

1. A predictive question; the campaign is interesting in predicting vote based on other information. 

2. An inferential question; the campaign has a hypothesis that they are testing, and they are interested in whether change in a specific predictor variable causes a change in the outcome. 